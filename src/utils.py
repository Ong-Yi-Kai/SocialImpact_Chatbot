import openai
import streamlit as st

def find_match(input, model, index, num_docs=5):
    input_em = model.encode(input).tolist()
    print(f"Length of embedding: {len(input_em)}")
    print(f"Length of embedding: {len(input_em)}")
    result = index.query(input_em, top_k=num_docs, includeMetadata=True)

    # join the top contexts together
    contexts = []
    for i in range(num_docs):
        contexts.append(result['matches'][i]['metadata']['text'])
    return "\n".join(contexts)


def query_refiner(conversation, query):
# The 'query_refiner' function takes the user's query and refine it
# to ensure it's optimal for providing a relevant answer. Uses OpenAI's DaVinci model
# to refine the query based on the current conversation log.

    response = openai.Completion.create(
    model="text-davinci-003",
    prompt=f"Given the following user query and conversation log, formulate a question that would be the most relevant\
to provide the user with an answer from a knowledge base.\n\nCONVERSATION LOG: \n{conversation}\n\nQuery:\
 {query}\n\nRefined Query:",
    temperature=0.7,
    max_tokens=256,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
    )

    return response['choices'][0]['text']

def get_conversation_string():
    conversation_string = ""
    for i in range(len(st.session_state['responses'])-1):

        conversation_string += "Human: "+st.session_state['requests'][i] + "\n"
        conversation_string += "Bot: "+ st.session_state['responses'][i+1] + "\n"
    return conversation_string